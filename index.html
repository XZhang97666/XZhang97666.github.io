<!-- <!DOCTYPE html> -->
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Weizhi Wang's Homepage</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le styles -->
    <link href="./assets_files/bootstrap.min.css" rel="stylesheet">
    <link href="./assets_files/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="./assets_files/yangqing.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
    <script src="assets/js/html5shiv.js"></script>
    <![endif]-->
    <link rel="icon" href="./assets_files/weizhi.ico">
</head>

<div class="visible-phone" id="blackBar">
    <a href="#top">About</a>
    <!--<a href="#research">Research</a>-->
    <a href="#publications">Publications</a>
    <a href="#experience">Experience</a>
    <a href="#awards">Awards</a>
    <a href="#talks">Talks</a>
    <!--<a href="#teaching">Teaching</a>-->
    <a target="_blank"
       href="./additional_files/CV_Weizhi_Wang.pdf">CV</a>
</div>

<body>

<div class="container span3 hidden-phone">
    <div id="floating_sidebar" class="span3">
        <!-- We use a fancy nav bar if there is enough space -->
        <!--<hr class="hidden-phone">-->
        <br>
        <ul class="nav nav-list bs-docs-sidenav hidden-phone">
            <li><a href="#top">About</a></li>
            <!--<li><a href="#research">Research</a></li>-->
            <li><a href="#publications">Publications</a></li>
            <li><a href="#experience">Experience</a></li>
            <li><a href="#awards">Awards</a></li>
            <li><a href="#talks">Talks</a></li>
            <li><a target="_blank"
                   href="./additional_files/CV_Weizhi_Wang.pdf">CV</a>
            </li>
        </ul>
        <hr class="hidden-phone">
        <div class="text-center hidden-phone">
            <img src="assets_files/me.jpg" alt="photo" class="logo-image">
            <br><br>
            weizhiwang AT ucsb DOT edu <br>
        </div>

        <!-- Otherwise, we simply use a flat list of links -->

    </div>
</div>


<div class="container">

    <div class="row">

        <div class="span9">
            <br>
            <h3>
                Weizhi Wang (王伟志)
            </h3>
            <h5>
                weizhiwang AT ucsb DOT edu
            / <a target="_blank"
                 href="https://www.linkedin.com/in/weizhiwang">LinkedIn</a>
            / <a target="_blank" 
                 href="https://github.com/Victorwz">GitHub</a>
            / <a target="_blank" 
                 href="https://scholar.google.com/citations?user=UC2_V1MAAAAJ">Google Scholar</a>
            / <a target="_blank"
                 href="./additional_files/CV_Weizhi_Wang.pdf">CV</a>
<!--             / <mark>actively looking for job</mark> -->
            </h5>
            <!-- Do I want to show a pic on the phone screen?
            <div class="text-center visible-phone">
                <img src="assets/img/Yihui.png" alt="photo" width="150px"/>
            </div>
            -->
            <a class="visible-phone pull-left" href="http://daggerfs.com/index.html#">
                <img class="media-object" src="assets_files/me.jpg" width="96px" style="margin: 0px 10px">
            </a>

            <p>
                I'm a final-year Ph.D. candidate advised by Prof. <a href="https://sites.cs.ucsb.edu/~xyan/index.htm" target="_blank">Xifeng Yan</a> in University of California Santa Barbara. Before that, I obtained my master and bachelor degree from Rutgers University and Xi'an Jiaotong University, respectively. 
            </p>
            <p>
                I'm on the Job Market!
            </p>

            <!--
             *** Research ***
            -->
            <h3>
            <a name="research"></a> Research Focus
            </h3>
            <p>
            I am focusing on the field of natural language processing. My research focus and past experience can be summarized as the following aspects:
            </p><ul>
            <li><strong>Multimodal LLM Pre-Training</strong>: 
                <ul>
                    <li>Data-Centric Research about the High-Quality Pre-Training Data for MLLMs (<a href="https://arxiv.org/pdf/2403.02677">[MLM-Filter]</a> [UniFilter]);</li>
                    <li>Compute-Efficient Pre-Training of MLLMs (<a href="https://arxiv.org/pdf/2504.00595">[Open-Qwen2VL] </a>)</li>
                    <li>Novel Architecture of MLLMs (<a href="https://arxiv.org/abs/2205.10178">[VaLM]</a>)</li>
                </ul>
                  </li> 

            <li><strong>Agentic Long-Term Memory</strong>: Augmenting LLMs or LM Agents with Long-Term Memory (<a href="https://arxiv.org/abs/2306.07174">[LongMem]</a>)
            </li>
            <li><strong>Efficient Inference of MLLMs</strong>: Adaptive Layer-Skipping of LLMs (<a href="https://arxiv.org/pdf/2503.23798">FlexiDepth</a>) </li>

            </ul>
            <p>Additionally, I also worked on some application areas before LLM Era like:
                <ul>
                    <li><strong>Multilingual Machine Translation and Speech-to-Text Translation</strong> <a href="">[EMNLP 22]</a> <a href="">[AAAI 22]</a> <a href="">[EMNLP 21]</a></li>
                    <li><strong>Dialogue Systems and AI Chatbot Agents</strong> <a href="">[GauchoChat]</a> <a href="">[SIGIR 22]</a></li>
                </ul>
            </p>
            <!-- <p> (Most recent publications to be added) </p> -->

            <h3>
                <a name="publications"></a> Preprints
            </h3>
            <div class="media">
                <!-- <a name="softer" class="pull-left">
                    <img class="media-object" src="assets_files/aaai2022_tda.png" width="96px" height="96px">
                </a> -->

                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources
                        </strong><br>
                          <strong>Weizhi Wang</strong>, Yu Tian, Linjie Yang, Heng Wang, Xifeng Yan<br>
                        <a target="_blank"
                           href="https://arxiv.org/pdf/2504.00595">[arXiv]</a>
                        <a target="_blank"
                           href="https://victorwz.github.io/Open-Qwen2VL/">[project website]</a>
                        <a target="_blank"
                           href="https://github.com/Victorwz/Open-Qwen2VL">[code]</a>
                        <a target="_blank"
                           href="https://huggingface.co/weizhiwang/Open-Qwen2VL">[model]</a>
                        <a target="_blank"
                           href="https://huggingface.co/datasets/weizhiwang/Open-Qwen2VL-Data">[data]</a>                        
                    </p>
                </div>
            </div>


            <div class="media">
                <!-- <a name="softer" class="pull-left">
                    <img class="media-object" src="assets_files/aaai2022_tda.png" width="96px" height="96px">
                </a> -->

                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Adaptive Layer-skipping in Pre-trained LLMs
                        </strong><br>
                          Xuan Luo, <strong>Weizhi Wang</strong>, Xifeng Yan <br>
                        <a target="_blank"
                           href="https://arxiv.org/pdf/2503.23798">[arXiv]</a>
                        <!-- <a target="_blank"
                           href="https://mlm-filter.github.io/">[project website]</a> -->
                        <!-- <a target="_blank"
                           href="https://github.com/Victorwz/MLM_Filter">[code]</a> -->
                        <a target="_blank"
                           href="https://huggingface.co/xuan-luo/FlexiDepth-Llama-3-8B-Instruct">[model]</a>
                        <a target="_blank"
                           href="https://huggingface.co/datasets/xuan-luo/FlexiPatterns-Llama-3-8B-Instruct">[data]</a>                        
                    </p>
                </div>
            </div>

            <div class="media">
                <!-- <a name="softer" class="pull-left">
                    <img class="media-object" src="assets_files/aaai2022_tda.png" width="96px" height="96px">
                </a> -->

                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters
                        </strong><br>
                          <strong>Weizhi Wang</strong>, Khalil Mrini, Linjie Yang, Sateesh Kumar, Yu Tian, Xifeng Yan, Heng Wang <br>
                        <a target="_blank"
                           href="https://arxiv.org/pdf/2403.02677.pdf">[arXiv]</a>
                        <a target="_blank"
                           href="https://mlm-filter.github.io/">[project website]</a>
                        <a target="_blank"
                           href="https://github.com/Victorwz/MLM_Filter">[code]</a>
                        <a target="_blank"
                           href="https://huggingface.co/weizhiwang/mlm-filter-llava-13b-gpt4v">[model]</a>
                        <a target="_blank"
                           href="https://huggingface.co/datasets/weizhiwang/mlm_filter_instructions">[data]</a>                        
                    </p>
                </div>
            </div>

            <div class="media">
                <!-- <a name="softer" class="pull-left">
                    <img class="media-object" src="assets_files/aaai2022_tda.png" width="96px" height="96px">
                </a> -->

                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks
                        </strong><br>
                           Xinlu Zhang*, Yujie Lu*, <strong>Weizhi Wang*</strong>, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Y. Wang, Linda R. Petzold  <br>
                        <a target="_blank"
                           href="https://arxiv.org/abs/2311.01361">[arXiv]</a>  (* Equal Contribution)  
                        <!-- <a target="_blank"
                           href="https://www.youtube.com/watch?v=bcGtNdTzdkc">[presentation]</a>  -->
                        <!-- <a target="_blank"
                           href="https://github.com/Victorwz/STEPS">[code]</a> -->
                        
                    </p>
                </div>
            </div>


            <div class="media">
                <!-- <a name="softer" class="pull-left">
                    <img class="media-object" src="assets_files/aaai2022_tda.png" width="96px" height="96px">
                </a> -->

                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            STEPS: A Benchmark for Order Reasoning in Sequential Tasks
                        </strong><br>
                            <strong>Weizhi Wang</strong>, Hong Wang, Xifeng Yan <br>
                        <a target="_blank"
                           href="https://arxiv.org/abs/2306.04441">[arXiv]</a>  
                        <!-- <a target="_blank"
                           href="https://www.youtube.com/watch?v=bcGtNdTzdkc">[presentation]</a>  -->
                        <a target="_blank"
                           href="https://github.com/Victorwz/STEPS">[code]</a>
                        
                    </p>
                </div>
            </div>
            
            <!--
             *** Publications ***
            -->
            <h3>
                <a name="publications"></a> Selected Publications
            </h3>

            <div class="media">
                <!-- <a name="softer" class="pull-left">
                    <img class="media-object" src="assets_files/aaai2022_tda.png" width="96px" height="96px">
                </a> -->

                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Bot or Human? Detecting ChatGPT Imposters with A Single Question
                        </strong><br>
                            Hong Wang, Xuan Luo, <strong>Weizhi Wang</strong>, Xifeng Yan <br>
                        <strong>COLM 2024.</strong>
                        <a target="_blank"
                           href="https://arxiv.org/abs/2305.06424">[arXiv]</a>  
                        <!-- <a target="_blank"
                           href="https://www.youtube.com/watch?v=bcGtNdTzdkc">[presentation]</a>  -->
                        <a target="_blank"
                           href="https://github.com/hongwang600/FLAIR">[code]</a>
                        
                    </p>
                </div>
            </div>
            
            
            <div class="media">
                <!-- <a name="softer" class="pull-left">
                    <img class="media-object" src="imgs/longmem.png" width="200px" height="200px">
                </a> -->
                
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Augmenting Language Models with Long-Term Memory
                        </strong><br>
                        <strong>Weizhi Wang</strong>, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei. <br>
                        <strong>NeurIPS 2023.</strong> <a target="_blank"
                           href="https://arxiv.org/abs/2306.07174">[arXiv]</a>  
                        <!-- <a target="_blank"
                           href="https://www.youtube.com/watch?v=bcGtNdTzdkc">[presentation]</a>  -->
                        <a target="_blank"
                           href="https://aka.ms/LongMem">[code]</a> <br>
                        Achievements: <br>
                        1. Awarded as <a href="https://twitter.com/susumuota/status/1669496359274045440">TOP 3 Popular ArXiv Papers on Twitter</a> during June 2023 <br>
                        2. Ranked <a href="https://github.com/yangwenmai/github-trending-backup/blob/master/2023/06/2023-06-16.md">No. 15 on Github Trending</a>  on June 16th, 2023, 774 Stars in total<br>
                        3. Media exposure <a href="https://mp.weixin.qq.com/s/LiWN7iONxgEOIPnJXjYgQw">AINLPer</a>, <a href="https://windowsreport.com/microsoft-longmem/">WindowsReport</a>, <a href="https://mp.weixin.qq.com/s/AVVsXnYvxGR5GJ5qO83RBw">GenAI</a>, <a href="https://yulleyi.medium.com/large-language-models-llm-with-long-term-memory-advancements-and-opportunities-in-genai-fcc3590f1c0e">Medium</a>, <a href="https://www.zdnet.com/article/microsoft-tiktok-give-generative-ai-a-sort-of-memory/">ZDNet</a>
                        
                    </p>
                </div>
            </div>

            <div class="media">
                <!-- <a name="softer" class="pull-left">
                    <img class="media-object" src="imgs/gauchochat.png" width="200px" height="200px">
                </a> -->

                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            GauchoChat: Towards Proactive, Controllable, and Personalized Social Conversation
                        </strong><br>
                        Hong Wang, <strong>Weizhi Wang</strong>, Rajan Saini, Marina Zhukova, Xifeng Yan<br>
                        <strong>Amazon Proceedings 2023.</strong>
                        <a target="_blank"
                        href="https://assets.amazon.science/80/c9/c241c7c94d09bfca04b4a73a4dc9/gauchochat-towards-proactive-controllable-and-personalized-social-conversation.pdf">[Paper]</a> <br>
                        <strong>Champion</strong>&#127942 for Amazon Alexa SocialBot Grand Challange 5.<br>
                        
                    </p>
                </div>
            </div>

            <div class="media">
                <!-- <a name="softer" class="pull-left">
                    <img class="media-object" src="imgs/valm.png" width="200px" height="200px">
                </a> -->

                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Visually-Augmented Language Modeling
                        </strong><br>
                            <strong>Weizhi Wang</strong>, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei. <br>
                        <strong>ICLR 2023.</strong>
                        <a target="_blank"
                           href="https://arxiv.org/abs/2205.10178">[arXiv]</a>  
                        <!-- <a target="_blank"
                           href="https://www.youtube.com/watch?v=bcGtNdTzdkc">[presentation]</a>  -->
                        <a target="_blank"
                           href="https://github.com/Victorwz/VaLM">[code]</a>
                        
                    </p>
                </div>
            </div>

        <div class="media">
            <!-- <a name="softer" class="pull-left">
                <img class="media-object" src="imgs/npdaknnst.png" width="200px" height="200px">
            </a> -->
            <div class="media-body">
                <p class="media-heading">
                    <strong>
                        Non-Parametric Domain Adaptation for End-to-End Speech Translation
                    </strong><br>
                    Yichao Du*, <strong>Weizhi Wang*</strong>, Zhirui Zhang, Boxing Chen, Tong xu, Jun Xie, Enhong Chen. <br>
                    <strong>EMNLP 2022. Oral Presentation, Top 3%. (* Equal Contribution) </strong>
                    <a target="_blank"
                       href="https://arxiv.org/abs/2205.11211">[arXiv]</a>  
                    <!-- <a target="_blank"
                       href="https://www.youtube.com/watch?v=bcGtNdTzdkc">[presentation]</a>                           -->
                    <a target="_blank"
                       href="https://github.com/duyichao/NPDA-KNN-ST">[code]</a>
                    
                </p>
            </div>
        </div>
        
        <div class="media">
            <!-- <a name="softer" class="pull-left">
                <img class="media-object" src="imgs/todnlg.png" width="200px" height="200px">
            </a> -->
            <div class="media-body">
                <p class="media-heading">
                    <strong>
                            Task-Oriented Dialogue Systems as Natural Language Generation
                    </strong><br>
                    <strong>Weizhi Wang</strong>, Zhirui Zhang, Junliang Guo, Yinpei Dai, Boxing Chen, Weihua Luo. <br>
                    <strong>SIGIR 2022.</strong>
                    <a target="_blank"
                        href="https://arxiv.org/abs/2108.13679">[arXiv]</a>  
                    <!-- <a target="_blank"
                        href="https://www.youtube.com/watch?v=bcGtNdTzdkc">[presentation]</a>                           -->
                    <a target="_blank"
                        href="https://github.com/Victorwz/tod_as_nlg">[code]</a>  
                </p>
            </div>
        </div>

            <div class="media">
                <!-- <a name="softer" class="pull-left">
                    <img class="media-object" src="imgs/tda.png" width="200px" height="200px">
                </a> -->
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Regularizing End-to-End Speech Translation with Triangular Decomposition Agreement
                        </strong><br>
                        Yichao Du, Zhirui Zhang, <strong>Weizhi Wang</strong>, Boxing Chen, Jun Xie, Tong Xu. <br>
                        <strong>AAAI 2022.</strong>
                        <a target="_blank"
                           href="https://arxiv.org/abs/2112.10991">[arXiv]</a>
                        <a target="_blank"
                           href="https://github.com/duyichao/E2E-ST-TDA">[code]</a>                                            
                    </p>
                </div>
            </div>

            <div class="media">
                <!-- <a name="fsaf" class="pull-left">
                    <img class="media-object" src="assets_files/emnlp2021_mnmt.png" width="200px" height="200px">
                </a> -->
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables
                     </strong><br>
                        <strong>Weizhi Wang</strong>, Zhirui Zhang, Yichao Du, Boxing Chen, Jun Xie, Weihua Luo. <br>
                        <strong>EMNLP Findings 2021.</strong>
                        <a target="_blank"
                           href="https://arxiv.org/abs/2109.04705">[arXiv]</a> 
                        <a target="_blank"
                           href="https://github.com/Victorwz/zs-nmt-dae">[code]</a>  
                    </p>
                    <!-- <p class="abstract-text">
                        We propose to introduce a denoising autoencoder objective based on pivot language into traditional training objective to improve the translation accuary on zero-shot directions. The theoretical analysis based on the perspective of latent variables shows that our proposed approach actually implicitly maximizes the probability distributions for zero-shot translation direction. 
                    </p> -->
                </div>
            </div>

            <!-- <div class="media">
                <a name="fsaf" class="pull-left">
                    <img class="media-object" src="assets_files/emnlp2021_mnmt.png" width="96px" height="96px">
                </a> 
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Blockchain-Based Botnets for Command-and-Control Resilience
                     </strong><br>
                        <strong>Weizhi Wang</strong>, Xiaobo Ma. <br>
                        <strong>Botnets. CRC Press. 2019.</strong>
                        <a target="_blank"
                           href="">[PDF]</a>  
                    </p>
                </div>
            </div> -->

            <!-- <div class="media">
                <a name="fsaf" class="pull-left">
                    <img class="media-object" src="assets_files/emnlp2021_mnmt.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Adaptive Region Growing For Unmanned System
                     </strong><br>
                     Tao Wang, Hui Cao, Xingyu Yan, Yanqing Ma, <strong>Weizhi Wang</strong>. <br>
                        <strong>CCC 2019.</strong>
                        <a target="_blank"
                           href="additional_files/Adaptive_Region_Growing_For_Unmanned_System.pdf">[PDF]</a>  
                    </p>
                </div>
            </div> -->

            <h3>
                <a name="experience"></a> Competition
            </h3>

            <div class="media">
                <!-- <a name="fsaf" class="pull-left">
                    <img class="media-object" src="assets_files/emnlp2021_mnmt.png" width="96px" height="96px">
                </a> -->
            
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            The Fifth Amazon Alexa Prize SocialBot Grand Challenge. 2023.
                        </strong>
                     <br>
                        <strong>Champion</strong>&#127942, winning $250,000 first place bonus <br>
                        
                        As a member of <i>GauchoChat</i>, we developed a proactive, controllable, and personalized social chatbot.
                        <a target="_blank"
                        href="hhttps://www.amazon.science/alexa-prize/proceedings/gauchochat-towards-proactive-controllable-and-personalized-social-conversation">[Paper]</a> 
                        <a target="_blank"
                            href="https://www.amazon.science/alexa-prize/socialbot-grand-challenge/2022">[News]</a>  
                    </p>
                </div>
            
            </div>

            <h3>
                <a name="experience"></a> Experience
            </h3>

            <div class="media">
                <!-- <a name="fsaf" class="pull-left">
                    <img class="media-object" src="assets_files/emnlp2021_mnmt.png" width="96px" height="96px">
                </a> -->
                <div class="media-body">
                    <p class="media-heading">
                    <strong>
                        Stores-Foudational AI, Amazon, Palo Alto, US. Jun 2024--Sep 2024
                     </strong>
                     <br>
                     Applied Scientist Intern
                     <br>
                     Mentor: <a href="https://rmlin.github.io/" target="_blank">Rongmei Lin</a>
                    </p>
                </div>

                <div class="media-body">
                    <p class="media-heading">
                    <strong>
                        Seed Vision Team, ByteDance, San Jose, US. Sep 2023--Feb 2024
                     </strong>
                     <br>
                     Research Intern
                     <br>
                     Mentor: <a href="https://hengcv.github.io/" target="_blank">Heng Wang</a>
                    </p>
                </div>
            
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Natural Language Computing Group, Microsoft Research Asia, Beijing, CN. Nov 2021--May 2023
                        </strong>
                     <br>
                     Research Intern, MSR Asia-MSR Redmond Joint StarLeap Program
                     <br>
                     Mentor: <a href="http://dong.li/" target="_blank">Li Dong</a>, <a target="_blank" href="https://sites.google.com/site/hcheng2site/Home">Hao Cheng</a>, <a target="_blank" href="https://sites.google.com/view/buptxiaodong/home">Xiaodong Liu</a>, <a target="_blank" href="http://gitnlp.org/">Furu Wei</a>
                    </p>
                </div>
            
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                        Machine Translation Group, Alibaba DAMO Academy, Hangzhou, CN. Sep 2020--Nov 2021
                        </strong>
                        <br>
                        Research Intern
                        <br>
                        Mentor: <a target="_blank" href="https://zrustc.github.io/">Zhirui Zhang</a> and <a target="_blank" href="https://sites.google.com/site/chenboxing/Home">Boxing Chen</a>
                    </p>
                </div>
               
            </div>
            <h3>
                <a name="awards"></a> Awards
            </h3>

            <div class="media">
                <div class="media-body">
                    <!-- <p class="media-heading"> -->
                        <ul class="bs-docs-sidenav hidden-phone">
                            <li>"Stars of Tomorrow" Excellent Intern Award, Microsoft Research, 2023.</li>
                            <li>Academic Excellence Fellowship. University of California Santa Barbara. 2022.</li>
                            <li>Outstanding Graduate Academic Performance Award. Rutgers University. 2021.</li>
                        </ul>   
                    <!-- </p> -->
                    <!-- <p><strong>Teaching Assistant</strong> </p>
                    <p class="abstract-text">
                        Working with Prof. <a href="http://www.ftrees.com/">Frances Trees </a> and other excellent members in teaching team to provide high quality course on 
                        front-end development using HTML5, CSS, and JavaScript.
                    </p> -->
                </div>
            </div>

            <h3>
                <a name="talks"></a> Talks
            </h3>

            <div class="media">
                <!-- <a name="fsaf" class="pull-left">
                    <img class="media-object" src="assets_files/emnlp2021_mnmt.png" width="96px" height="96px">
                </a> -->
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Revisiting GPT-Series and InstructGPT. 
                     </strong>
                     <strong>Talk at UCSB CS291A.</strong> <a target="_blank" href="additional_files/slides_gpt_cs291A.pdf">[Slides]</a>  
                    </p>
                </div>

                <!-- <a name="fsaf" class="pull-left">
                    <img class="media-object" src="assets_files/emnlp2021_mnmt.png" width="96px" height="96px">
                </a> -->
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            CLIP and VaLM. 
                     </strong>
                     <strong>Talk at UCSB CS291A.</strong> <a target="_blank" href="additional_files/slides_clip_cs291A.pdf">[Slides]</a>  
                    </p>
                </div>
            </div>
            
            <h3>
                <a name="experience"></a> Professional Services
            </h3>

            <div class="media">
                <!-- <a class="pull-left">
                    <img class="media-object" src="./assets_files/cs188.png" width="96px" height="96px">
                </a> -->
                <div class="media-body">
                    <p class="media-heading">
                        <strong>- Conferences Reviewer: ACL Rolling Review, KDD 2023, NeurIPS 2023-2024, ICLR 2024, SDM 2024. ICML 2024.<br>
                            - Journal Reviewer: TASLP, TALLIP.</strong>
                        
                    </p>
                    <!-- <p><strong>Teaching Assistant</strong> </p>
                    <p class="abstract-text">
                        Working with Prof. <a href="http://www.ftrees.com/">Frances Trees </a> and other excellent members in teaching team to provide high quality course on 
                        front-end development using HTML5, CSS, and JavaScript.
                    </p> -->
                </div>
            </div>

            <!-- <h3>
                <a name="talks"></a> Projects
            </h3> -->
            <!-- Link to my <a target="_blank" href="https://github.com/Victorwz">[github public projects]</a> -->

            <!-- <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="assets_files/htssa.png"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Hierarchical Fine-grained Sentiment Classification Using BERT
                        </strong>
                    </p>
                    <p class="abstract-text">
                    We propose a hierarchical tree-structured sentiment classification model conjunction with the data augmentation approach, to solve the data imbalance problem in fine-grained sentiment classification. 
                    The proposed model deploys a tree-structured classification architecture, with every non-leaf tree node as a pre-trained BERT model. The proposed method can automatically transform fine-grained sentiment classification into hierarchical bi-classification or tri-classification tasks and complete the task through a decision tree structure. 
                    The experimental results demonstrate that proposed method improves the fine-grained classification performance of BERT-base baseline by 3.5%.
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="assets_files/gridcell.png"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Generative Hippocampal-entorhinal System
                        </strong>
                        <a target="_blank" href="https://github.com/Victorwz/Generative-Hippocampal-entorhinal-System">[Code]</a>
                    </p>
                    <p class="abstract-text">
                        We build up a generative model and inference network with VAE to intimidate the grid cells and place cells in Hippocampal-entorhinal System of human.
                    </p>
                </div>
            </div> -->

            <!-- <div class="media">
                <a class="pull-left">
                    <img class="media-object" 
                         src="assets_files/acc.png"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Fast Weights
                        </strong>
                        <a target="_blank" href="https://github.com/Victorwz/fast-weights-pytorch">[Code]</a>
                    </p>
                    <p class="abstract-text">
                        I reimplemented the paper <a target="_blank" href="https://arxiv.org/abs/1610.06258.pdf">[Using Fast Weights to Attend to the Recent Past]</a> using PyTorch. The other public available reimplementations for this paper are using TensorFlow. The reimplemented accuracy of the information retrieval task in the source paper achieves 98%.
                    </p>
                </div>
            </div> -->

            <!-- <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="assets_files/crf.png"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Awesome NLP Models
                        </strong>
                        <a target="_blank" href="https://github.com/Victorwz/awesome_NLP_models">[Code]</a>
                    </p>
                    <p class="abstract-text">
                        I reimplemented several classic NLP models using PyTorch, including HMM, Self-Attention, CRF, Log-Linear, Ngram.
                    </p>
                </div>
            </div> -->

            

            <h3>
                <a name="teaching"></a> Teaching
            </h3>

            <div class="media">
                <!-- <a class="pull-left">
                    <img class="media-object" src="./assets_files/cs188.png" width="96px" height="96px">
                </a> -->
                <div class="media-body">
                    <p class="media-heading">
                        <strong>Rutgers CS170: Computer Application for Business, Teaching Assistant, Spring 2020 & Fall 2020 & Spring 2021.</strong>
                        <a target="_blank"
                                   href="https://www.cs.rutgers.edu/academics/undergraduate/course-synopses/course-details/01-198-170-computer-application-for-business">[Course Website]</a>
                    </p>
                    <!-- <p><strong>Teaching Assistant</strong> </p>
                    <p class="abstract-text">
                        Working with Prof. <a href="http://www.ftrees.com/">Frances Trees </a> and other excellent members in teaching team to provide high quality course on 
                        front-end development using HTML5, CSS, and JavaScript.
                    </p> -->
                </div>
                <!-- <a class="pull-left">
                    <img class="media-object" src="./assets_files/cs188.png" width="96px" height="96px">
                </a> -->
                <div class="media-body">
                    <p class="media-heading">
                        <strong>UCSB CS165A: Artificial Intelligence, Teaching Assistant, Fall 2022.</strong>
                        <a target="_blank"
                                   href="https://sites.cs.ucsb.edu/~xyan/classes/CS165A-2022fall/">[Course Website]</a>
                    </p>
                    <!-- <p><strong>Teaching Assistant</strong> </p>
                    <p class="abstract-text">
                        Working with Prof. <a href="http://www.ftrees.com/">Frances Trees </a> and other excellent members in teaching team to provide high quality course on 
                        front-end development using HTML5, CSS, and JavaScript.
                    </p> -->
                </div>
            </div>
            <!-- Footer
            ================================================== -->
            <hr>
            <!-- <footer class="footer">
                <div class='hidden-phone'>
                <h3 class="text-center"><a name="wall"></a><strong>works</strong></h3>
                <section id="photos">
                    <img src="https://raw.githubusercontent.com/yihui-he/lip-tracking-with-snake-active-contour-and-particle-filter/master/pic.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/Edge-detection-with-zero-crossing/master/lena_1.bmp"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/3D-reconstruction/master/result/selfff.png"/>
                    <img src="./assets_files/cs188.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/person.jpg.png"/>
                    <img src="./assets_files/vehicle.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/Depth-estimation-with-neural-network/master/presentation/stereo.png?token=AJkBS_A-YWaMd9vcgEQuaXQWe9wmjtTBks5XWM07wA%3D%3D"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/resnet.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/kmeans.jpg"/>
                    <img src="./assets_files/shuttle.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/gaoxin.jpg"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/artwork.jpg"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/bop.jpg"/>
                    <img src="./assets_files/ocsi.png"/>
                </section>
                
                <a target="_blank" href="https://github.com/yihui-he/panorama"><img
                        src="https://github.com/yihui-he/panorama/blob/master/results/yellowstone5.jpg?raw=true"></a>
                <hr>
                </div> -->
                <div class="row">
                    <div class="span12">
                        <p>
                            modified from <a target="_blank" href="http://daggerfs.com/">© Yangqing Jia 2013</a>
                        </p>
                    </div>
                </div>
                <div class="busuanzi-count">
                    Total Visits:
                    <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
                    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
                    <span class="post-meta-item-icon">
                    <!-- <i class="fa fa-eye"></i> -->
                    </span>
                    <span class="site-pv" title="total visit times">
                    <span id="busuanzi_value_site_pv"></span>
                    </span>
                    </span>
                    </div>
                <br>
            </footer>
        </div>
    </div>
</div>
</body>
</html>

<!-- Le javascript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<!--
    <script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
    <script src="assets/js/bootstrap-transition.js"></script>
    <script src="assets/js/bootstrap-alert.js"></script>
    <script src="assets/js/bootstrap-modal.js"></script>
    <script src="assets/js/bootstrap-dropdown.js"></script>
    <script src="assets/js/bootstrap-scrollspy.js"></script>
    <script src="assets/js/bootstrap-tab.js"></script>
    <script src="assets/js/bootstrap-tooltip.js"></script>
    <script src="assets/js/bootstrap-popover.js"></script>
    <script src="assets/js/bootstrap-button.js"></script>
    <script src="assets/js/bootstrap-collapse.js"></script>
    <script src="assets/js/bootstrap-carousel.js"></script>
    <script src="assets/js/bootstrap-typeahead.js"></script>
    <script src="assets/js/bootstrap-affix.js"></script>
    <script src="assets/js/holder/holder.js"></script>
    <script src="assets/js/application.js"></script>
-->
